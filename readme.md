# ğŸ§ Vasha-AI â€” Real-Time AI Speech Translation System

> ğŸ§  **Vasha-AI** is an intelligent multilingual pipeline that performs **real-time speech-to-speech translation** across 200+ global and Indic languages.
> It integrates **Automatic Speech Recognition (ASR)**, **Language Identification (LID)**, and **Machine Translation (MT)** with features like **NER-preservation**, **code-mixed handling**, **transliteration**, and **spoof detection**.

---

## ğŸŒ Key Features

âœ… **Real-Time Language Identification (LID)**
âœ… **Automatic Speech Recognition (ASR)** with:

* OpenAI **Whisper**
* AI4Bharat **IndicConformer**
* **Faster-Whisper** (batched inference)

âœ… **Machine Translation (MT)** with:

* Meta **NLLB (No Language Left Behind)**
* AI4Bharat **IndicTrans2**
* **Google Translate** API wrapper

âœ… **Smart Preprocessing**:

* Named Entity Preservation (NER)
* Script Transliteration
* Code-Mixed Text Handling (e.g., Hinglish)

âœ… **Advanced Debugging**:

* Back-Translation Consistency Check
* NER Preservation Mode
* Transliteration-only Mode

âœ… **Security**:

* Spoof Detection for fake audio
* Dialect tagging (Hindi, Tamil, Bengali, etc.)

âœ… **UX Improvements**:

* Real-time progress bars (`tqdm`)
* Session-wise results saved locally

---

## ğŸ§© Project Directory Structure

```
Vasha-AI/
â”‚
â”œâ”€â”€ ASR_Model/
â”‚   â”œâ”€â”€ faster-whisper/              # Faster-Whisper backend
â”‚   â”œâ”€â”€ indic_conformer/             # AI4Bharat IndicConformer ASR
â”‚   â”‚   â”œâ”€â”€ conformer_asr.py
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ whisper/                     # Whisper wrapper
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ LID_Model/
â”‚   â”œâ”€â”€ lid.py                       # Language ID + dialect detection
â”‚   â”œâ”€â”€ spoof_detection.py           # Spoof detection
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ MT_Model/
â”‚   â”œâ”€â”€ IndicTrans2/                 # AI4Bharat IndicTrans2 model
â”‚   â”œâ”€â”€ nllb-3.3B/                   # Meta NLLB 3.3B model weights
â”‚   â”œâ”€â”€ Open-NLLB/                   # Optional fine-tuned NLLB variant
â”‚   â”œâ”€â”€ mt_model.py                  # Unified translation model loader
â”‚   â”œâ”€â”€ mt_helper.py                 # Menu + progress bar integration
â”‚   â”œâ”€â”€ mt_google.py                 # Google Translate API
â”‚   â”œâ”€â”€ mt_preprocessor.py           # NER, transliteration, code-mix logic
â”‚   â”œâ”€â”€ mt_debug.py                  # Back-translation utilities
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ Audio_TestWAV/                   # Sample WAVs
â”œâ”€â”€ sessions/                        # Saved transcriptions & translations
â”œâ”€â”€ youtube_cache/                   # Cached YouTube WAVs
â”‚
â”œâ”€â”€ transcribe_pipeline.py           # Main entry-point script
â”œâ”€â”€ gpusage.py                       # GPU usage tracker
â”œâ”€â”€ recorded.wav                     # Sample audio
â”œâ”€â”€ requirements.txt                 # Global dependencies
â””â”€â”€ README.md                        # You're reading this file
```

---

## ğŸ§  System Architecture

```
ğŸ¤ Audio Input
   â”‚
   â”œâ”€â”€â–º Language Identification (LID_Model)
   â”‚      â”œâ”€â”€ Language & Dialect Detection
   â”‚      â””â”€â”€ Spoof Detection (Fake vs Real)
   â”‚
   â”œâ”€â”€â–º Automatic Speech Recognition (ASR_Model)
   â”‚      â”œâ”€â”€ Whisper / Faster-Whisper / IndicConformer
   â”‚      â””â”€â”€ Converts Speech â†’ Text
   â”‚
   â”œâ”€â”€â–º Machine Translation (MT_Model)
   â”‚      â”œâ”€â”€ NLLB / IndicTrans2 / Google
   â”‚      â”œâ”€â”€ Transliteration / Code-mixed / NER-Preserve
   â”‚      â””â”€â”€ Progress bar + batching (tqdm)
   â”‚
   â””â”€â”€â–º Output
          â”œâ”€â”€ Translated Text
          â”œâ”€â”€ Saved Transcription Files
          â””â”€â”€ Optional Back-Translation Debug
```

---

## âš™ï¸ Installation

### 1ï¸âƒ£ Clone the repository

```bash
git clone https://github.com/<your-username>/Vasha-AI.git
cd Vasha-AI
```

### 2ï¸âƒ£ Create a new environment

```bash
conda create -n lid-env python=3.10 -y
conda activate lid-env
```

### 3ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ (Optional) Install extra packages for IndicTrans2 & NLLB

```bash
pip install torch torchvision torchaudio
pip install transformers sentencepiece sacremoses accelerate
```

---

## ğŸ¤ Running the Pipeline

### â–¶ï¸ Real-Time Microphone Translation

```bash
python transcribe_pipeline.py --mic --duration 10
```

### â–¶ï¸ Translate YouTube Videos

```bash
python transcribe_pipeline.py --youtube https://youtu.be/<video_id>
```

### â–¶ï¸ Process a Local File

```bash
python transcribe_pipeline.py --file sample_video.mp4
```

---

## ğŸ§ª Debugging Options

| Option                   | Description                              |
| ------------------------ | ---------------------------------------- |
| **1**                    | Normal Translation                       |
| **2**                    | Batch Translation (with progress bar)    |
| **3**                    | Back-Translation Debug                   |
| **4**                    | NER-Preservation Mode                    |
| **Transliteration Mode** | Converts script without changing meaning |
| **Code-Mixed Mode**      | Handles bilingual runs like Hinglish     |

---

## ğŸ§¾ Example Output

```
ğŸ•£ Transcribed Text:
 à¤¨à¤®à¤¸à¥à¤¤à¥‡ à¤®à¥‡à¤°à¤¾ à¤¨à¤¾à¤® à¤¦à¥€à¤ª à¤¹à¥ˆ...

ğŸ’¬ Translation (Hindi â†’ English):
 Hello, my name is Deep...

ğŸ’¾ Saved to:
 sessions/session_20251027_142512/translation_hi_to_eng_Latn.txt
```

---

## ğŸ’¡ Advanced Features

| Feature              | Description                                        |
| -------------------- | -------------------------------------------------- |
| **Progress Bar**     | `tqdm` integrated into translation chunks          |
| **NER Preservation** | Keeps named entities (people, places) untranslated |
| **Back-Translation** | Validates translation consistency                  |
| **Spoof Detection**  | Flags fake/AI-generated voices                     |
| **GPU Monitoring**   | Optional `gpusage.py` shows CUDA stats             |

---

## ğŸ¦‰ Example Session Folder

```
sessions/session_20251027_142512/
â”œâ”€â”€ output_hi_whisper.txt
â”œâ”€â”€ translation_hi_to_eng_Latn.txt
â””â”€â”€ debug_log.txt
```

---

## ğŸ§‘â€ğŸ’» Development Notes

* Each module is fully independent (LID, ASR, MT).
* Uses **Whisper** or **IndicConformer** dynamically based on detected language.
* **Meta NLLB** is default global MT model; falls back to **IndicTrans2** for Indic pairs.
* Integrated **tqdm progress bar** for smoother user experience during long translations.

---

## ğŸ”‹ Requirements

```
torch
torchaudio
transformers
tqdm
sentencepiece
sacremoses
faster-whisper
indic-nlp-library
langid
spacy
flask
pydub
openai-whisper
```

---

## ğŸ§± Future Enhancements

* Real-time subtitle overlay for YouTube/Twitch streams
* Flask/FastAPI dashboard
* Speaker diarization
* Multi-GPU batch processing
* Offline IndicTrans2 quantization

---

## ğŸ‘¨â€ğŸ’» Author

**Deep Habiswashi**
**Soumyadeep Dutta**


---

## ğŸª„ License

```
MIT License Â© 2025 Deep Habiswashi
```



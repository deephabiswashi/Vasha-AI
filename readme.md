# ğŸ§ Vasha-AI â€” Real-Time AI Speech Translation System

> ğŸ§  **Vasha-AI** is an intelligent multilingual pipeline that performs **real-time speech-to-speech translation** across 200+ global and Indic languages.
> It integrates **Automatic Speech Recognition (ASR)**, **Language Identification (LID)**, **Machine Translation (MT)**, and **Text-to-Speech (TTS)** with features like **NER-preservation**, **code-mixed handling**, **transliteration**, and **spoof detection**.

This repository hosts the **core ASR/LID/MT/TTS models and pipeline** that power the production Vasha web experience.

---

## ğŸ“º Demo Video

You can watch a short **demo of Vasha-AI in action** here:  
[Vasha-AI Demo on YouTube](https://youtu.be/16qauRDXsOg)

---

## ğŸŒ Key Features

âœ… **Real-Time Language Identification (LID)**
âœ… **Automatic Speech Recognition (ASR)** with:

* OpenAI **Whisper**
* AI4Bharat **IndicConformer**
* **Faster-Whisper** (batched inference)

âœ… **Machine Translation (MT)** with:

* Meta **NLLB (No Language Left Behind)**
* AI4Bharat **IndicTrans2**
* **Google Translate** API wrapper

âœ… **Text-to-Speech (TTS)** via a unified backend:

* Indic-Parler (Indic voices)
* Coqui **XTTS**
* **gTTS** with caching & chunking

âœ… **Smart Preprocessing**:

* Named Entity Preservation (NER)
* Script Transliteration
* Code-Mixed Text Handling (e.g., Hinglish)

âœ… **Advanced Debugging**:

* Back-Translation Consistency Check
* NER Preservation Mode
* Transliteration-only Mode

âœ… **Security**:

* Spoof Detection for fake audio
* Dialect tagging (Hindi, Tamil, Bengali, etc.)

âœ… **UX Improvements**:

* Real-time progress bars (`tqdm`)
* Session-wise results saved locally

---

## ğŸ§© Project Directory Structure

```
Vasha-Models/
â”‚
â”œâ”€â”€ ASR_Model/
â”‚   â””â”€â”€ indic_conformer/
â”‚       â””â”€â”€ conformer_asr.py         # AI4Bharat IndicConformer ASR wrapper
â”‚
â”œâ”€â”€ LID_Model/
â”‚   â”œâ”€â”€ lid.py                       # Language ID + dialect detection
â”‚   â”œâ”€â”€ spoof_detection.py           # Spoof detection
â”‚   â”œâ”€â”€ requirements.txt
â”‚
â”œâ”€â”€ MT_Model/
â”‚   â”œâ”€â”€ mt_model.py                  # Unified translation model loader (NLLB, etc.)
â”‚   â”œâ”€â”€ mt_helper.py                 # Menu + progress bar integration
â”‚   â”œâ”€â”€ mt_google.py                 # Google Translate API
â”‚   â”œâ”€â”€ mt_preprocessor.py           # NER, transliteration, code-mix logic
â”‚   â”œâ”€â”€ mt_debug.py                  # Back-translation utilities
â”‚   â””â”€â”€ nllb-3.3B/                   # Meta NLLB model assets
â”‚       â”œâ”€â”€ README.md
â”‚       â””â”€â”€ sentencepiece.bpe.model
â”‚
â”œâ”€â”€ TTS_Model/
â”‚   â”œâ”€â”€ tts_common/                  # Shared TTS interface & utilities
â”‚   â””â”€â”€ tts_cache/                   # Cached synthesized audio
â”‚
â”œâ”€â”€ diagrams/                        # Architecture & speech-translation flowcharts
â”œâ”€â”€ output_tts/                      # Example synthesized waveforms
â”‚
â”œâ”€â”€ transcribe_pipeline.py           # Main end-to-end pipeline script
â”œâ”€â”€ gpusage.py                       # GPU usage tracker
â”œâ”€â”€ requirements.txt                 # Global dependencies
â””â”€â”€ readme.md                        # You're reading this file
```

---

## ğŸ§  System Architecture

```
ğŸ¤ Audio Input
   â”‚
   â”œâ”€â”€â–º Language Identification (LID_Model)
   â”‚      â”œâ”€â”€ Language & Dialect Detection
   â”‚      â””â”€â”€ Spoof Detection (Fake vs Real)
   â”‚
   â”œâ”€â”€â–º Automatic Speech Recognition (ASR_Model)
   â”‚      â”œâ”€â”€ Whisper / Faster-Whisper / IndicConformer
   â”‚      â””â”€â”€ Converts Speech â†’ Text
   â”‚
   â”œâ”€â”€â–º Machine Translation (MT_Model)
   â”‚      â”œâ”€â”€ NLLB / IndicTrans2 / Google
   â”‚      â”œâ”€â”€ Transliteration / Code-mixed / NER-Preserve
   â”‚      â””â”€â”€ Progress bar + batching (tqdm)
   â”‚
   â””â”€â”€â–º Output
          â”œâ”€â”€ Translated Text
          â”œâ”€â”€ Saved Transcription Files
          â””â”€â”€ Optional Back-Translation Debug
```

---

## ğŸ–¼ Architecture & Flowcharts

![Vasha model pipeline diagram](diagrams/model_pipeline.png)

This diagram shows the **high-level model pipeline**, starting from raw audio input and flowing through LID, ASR, MT, and optional TTS to produce translated speech and text.

![End-to-end speech translation diagram](diagrams/speech_translation.png)

This diagram focuses on the **end-to-end speech translation experience**, illustrating how user audio moves through the backend services to the production web frontend.

---

## ğŸŒ Production Website & Frontend

- **Production web app repo**: [`vasha-website`](https://github.com/SOUMYADEEPDUTTACODER/vasha-website) â€” TypeScript + Vite frontend and Python backend wiring for deploying these models in production.  
- **Live website**: [`https://vasha-website.vercel.app/`](https://vasha-website.vercel.app/) â€” main Vasha AI experience powered by this models repository.

---

---

## ğŸ§© Vasha-AI Chrome Extension (New!)

The project now includes a powerful **Manifest V3 Chrome Extension** that brings Vasha-AI directly to your browser throughout the web.

### âœ¨ Extension Features
*   **Real-Time Tab Audio Capture**: Capture audio from YouTube, Twitch, Meet, or any HTML5 audio source.
*   **Live Translation Overlay**: View translated text in real-time within the extension popup.
*   **Visual LID Timeline**: A dynamic color-coded timeline that visualizes language changes (e.g., Hindi â†’ English â†’ Bengali) in real-time.
*   **Mixed Mode**: Capture both **Tab Audio** and **Microphone** simultaneously for interviews or reaction videos.
*   **State Persistence**: Recording state and transaction history are saved even if the popup closes.

### ğŸš€ Setting up the Extension
1.  Open Chrome and navigate to `chrome://extensions`.
2.  Enable **Developer Mode** (toggle in top right).
3.  Click **Load Unpacked**.
4.  Select the `chrome_extension` directory from this repository.
5.  Pin the **Vasha-AI** icon to your toolbar.

### ğŸ® Using the Extension
1.  Start the backend server: `python vasha_server.py`.
2.  Open a tab with audio (e.g., a YouTube video).
3.  Click the Vasha extension icon.
4.  Select your **Target Language** (e.g., Hindi).
5.  Click **â–¶ Start Translating**.
6.  The extension will capture audio, identify the language, and display the translation instantly.

---

## âš™ï¸ Installation

### 1ï¸âƒ£ Clone the repository

```bash
git clone https://github.com/<your-username>/Vasha-AI.git
cd Vasha-AI
```

### 2ï¸âƒ£ Create a new environment

```bash
conda create -n lid-env python=3.10 -y
conda activate lid-env
```

### 3ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ (Optional) Install extra packages for IndicTrans2 & NLLB

```bash
pip install torch torchvision torchaudio
pip install transformers sentencepiece sacremoses accelerate
```

---

## ğŸ¤ Running the Pipeline

### â–¶ï¸ Real-Time Microphone Translation

```bash
python transcribe_pipeline.py --mic --duration 10
```

### â–¶ï¸ Translate YouTube Videos

```bash
python transcribe_pipeline.py --youtube https://youtu.be/<video_id>
```

### â–¶ï¸ Process a Local File

```bash
python transcribe_pipeline.py --file sample_video.mp4
```

---

## ğŸ§ª Debugging Options

| Option                   | Description                              |
| ------------------------ | ---------------------------------------- |
| **1**                    | Normal Translation                       |
| **2**                    | Batch Translation (with progress bar)    |
| **3**                    | Back-Translation Debug                   |
| **4**                    | NER-Preservation Mode                    |
| **Transliteration Mode** | Converts script without changing meaning |
| **Code-Mixed Mode**      | Handles bilingual runs like Hinglish     |

---

## ğŸ§¾ Example Output

```
ğŸ•£ Transcribed Text:
 à¤¨à¤®à¤¸à¥à¤¤à¥‡ à¤®à¥‡à¤°à¤¾ à¤¨à¤¾à¤® à¤¦à¥€à¤ª à¤¹à¥ˆ...

ğŸ’¬ Translation (Hindi â†’ English):
 Hello, my name is Deep...

ğŸ’¾ Saved to:
 sessions/session_20251027_142512/translation_hi_to_eng_Latn.txt
```

---

## ğŸ’¡ Advanced Features

| Feature              | Description                                        |
| -------------------- | -------------------------------------------------- |
| **Progress Bar**     | `tqdm` integrated into translation chunks          |
| **NER Preservation** | Keeps named entities (people, places) untranslated |
| **Back-Translation** | Validates translation consistency                  |
| **Spoof Detection**  | Flags fake/AI-generated voices                     |
| **GPU Monitoring**   | Optional `gpusage.py` shows CUDA stats             |

---

## ğŸ¦‰ Example Session Folder

```
sessions/session_20251027_142512/
â”œâ”€â”€ output_hi_whisper.txt
â”œâ”€â”€ translation_hi_to_eng_Latn.txt
â””â”€â”€ debug_log.txt
```

---

## ğŸ§‘â€ğŸ’» Development Notes

* Each module is fully independent (LID, ASR, MT).
* Uses **Whisper** or **IndicConformer** dynamically based on detected language.
* **Meta NLLB** is default global MT model; falls back to **IndicTrans2** for Indic pairs.
* Integrated **tqdm progress bar** for smoother user experience during long translations.

---

## ğŸ”‹ Requirements

```
torch
torchaudio
transformers
tqdm
sentencepiece
sacremoses
faster-whisper
indic-nlp-library
langid
spacy
flask
pydub
openai-whisper
```

---

## ğŸ§± Future Enhancements

* Real-time subtitle overlay for YouTube/Twitch streams
* Flask/FastAPI dashboard
* Speaker diarization
* Multi-GPU batch processing
* Offline IndicTrans2 quantization

---

## ğŸ‘¨â€ğŸ’» Author

**Deep Habiswashi**
**Soumyadeep Dutta**
**Sudeshna Mohanty**


---

## ğŸª„ License

```
MIT License Â© 2025 Deep Habiswashi
```


